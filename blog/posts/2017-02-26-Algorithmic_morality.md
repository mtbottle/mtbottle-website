---
title: Half-baked Idea for Algorithmic Morality
tags: philosophy, technology
---

Over the past several months,
I have read articles about generative learning 
and what happens when we let machines kind of *learn on their own*.
For example, the [Microsoft's Tay-bot](https://en.wikipedia.org/wiki/Tay_%28bot%29)
was an interesting case study into how to teach a bot all the *wrong* things.
Also, other articles (ie. [this](http://www.languagesoftheworld.info/translation/google-translates-gender.html)) 
talk about gender biases in Google Translate
and that the translation will infer gender bias from a gender neutral language to a gendered language.
The translation infers the gender based on what is salient in the corpus, 
which usually involves history of gender biases.

Machine learning scientists always get uncomfortable when comparing machine learning with "human" learning. 
It is not a completely unwarranted caution; 
machine learning is restricted in terms of its abilities 
and scope of problem solving. 
The algorithms tend to do one thing 
(usually a thing that is of *limited scope* and *well defined*, 
but perhaps not so well defined as to be clear what "rules" are), 
and usually does it well enough. 
For example, face detection and machine translation
have fairly clearly defined expected outcomes 
and the context in the classification usually entails preprocessed images or clean, well structured text.
The preprocessing is usually done by a human whose job is to "tweak" nature 
and make it easier for the machine to do better.
Human intelligence tends to be much more complex 
as we do not just the classification, 
but also all the relevance processing that machines take for granted.

Given that warning, 
my half-baked idea of teaching machine morality 
will overlook the caution that scientists have and 
use the analogy of how we teach morality to children for machines.

## How do we do this then?

So inspired by how we might teach children how to be moral
(and by thinking about how training happens on a neural network,
or some sort of multidimensional Bayesian convergence algorithm):

1. Young machines are *impressionable*, 
so we want to train the behaviours initially with unambiguous moral examples. 
For example, if we want to instill them the idea that theft is wrong,
these training data must be unambiguous cases where theft is unacceptable.
This will set the framework for the machine's moral system that will be very difficult to change later.

2. As the machine matures, we can start to instill more morally ambiguous example.
So using theft, this will involve cases where stealing is not entirely reprehensible. 
A relevant modern example could be: 
is it wrong to steal if the person is starving and do not have access to free foods?
It is also important that the trainer impose a solution or perspective to these morally ambiguous questions.
For example, we have to say that is it not wrong (or entirely wrong) for someone to steal
in these circumstances even though we know stealing in general is wrong.

3. Then expose the machines to other trainers with different perspectives.
Different value systems will give your machine a broader perspective on what's "right"
and possibly better answers for the morally ambiguous things. 
To do this, we can probably just get more training data with some more similar examples.

The same limitations for teaching children will exist in this framework.
So for example, the value system that the machine is initially trained in 
will bias it towards those that system of thought.
The analogy to that is that children are the product of the parents or community that they grew up in.
Another limitation is that the initial trainer is a very influencial trainer for the machine
and it will require a lot of effort and resources to correct the machine at the beginning. 
Analogously, it should be similar to the amount of effort it takes for a parent or mentor to teach a child.
With that in mind, teaching morality would be a *heavily* supervised type of learning.

What may be different is the scale of how to train these moral machines.
We only need to set up an initial framework that creates a unambiguous dataset of "morals"
then at certain points in the machine's training, 
start incorporating the ambiguous dataset and other people's training data.

Also, similar to human morality (when looking at it historically),
morality changes over time based on religion and culture.
In this case, religion and culture are analogous to the initial system in which the machine is taught.
Then self-derived morality (distinguishing the ambiguous) is built on top of that initial structure.
So in some ways, what an initial trainer might find "morally relevant" 
or unambiguous will change over time based on particular needs of the society.

Ultimately, I think morality is a very human concept that,
to codify correctly,
involves a deeper understanding of how humans learn morality.

## What does it say about *natural* morality?

Then the age old question: are humans naturally moral? 
(in this scenario, are computers naturally moral)
I am not very familiar with the literature around this, 
but my inclination is that:
we will *have* to assume that humans/machines are not naturally moral
and that it is taught for this to work.
Otherwise, if morally arises naturally, 
then one cannot be *taught* morality (and we cannot expect machines to either).
The arguments against machine morality might involve getting into 
definition wars with "what does it mean to be moral",
which I think is out of the scope of this half-baked article.
